{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Armandkay/fraud-detection-ml/blob/main/ML_Fraud_Detection_Enhanced.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a506681",
      "metadata": {
        "id": "8a506681"
      },
      "source": [
        "# ML-Powered Financial Fraud Detection System\n",
        "\n",
        "## Enhanced Model Notebook\n",
        "\n",
        "**Author:** Armand Kayiranga  \n",
        "**Project:** Machine Learning-Based Fraud Detection System  \n",
        "**Track:** ML Track - Initial Software Demo  \n",
        "\n",
        "---\n",
        "\n",
        "This notebook presents a comprehensive machine learning system for detecting fraudulent financial transactions. The project includes:\n",
        "\n",
        "1. **Data Visualization & Engineering**\n",
        "2. **Model Architecture Design**\n",
        "3. **Performance Metrics Evaluation**\n",
        "4. **Model Persistence for Deployment**\n",
        "\n",
        "The work presented here serves as the foundation for a deployed web-based fraud detection system."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "26sDqh6Ms39B"
      },
      "id": "26sDqh6Ms39B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "14442925",
      "metadata": {
        "id": "14442925"
      },
      "source": [
        "## 1. Dataset Description\n",
        "\n",
        "The dataset contains **10,000 simulated financial transactions** with the following features:\n",
        "\n",
        "**Features:**\n",
        "- `transaction_id`: Unique identifier\n",
        "- `amount`: Transaction amount in USD\n",
        "- `transaction_hour`: Hour of day (0-23)\n",
        "- `merchant_category`: Type of merchant (Electronics, Travel, Grocery, Food, Clothing)\n",
        "- `foreign_transaction`: Binary indicator (0=domestic, 1=foreign)\n",
        "- `location_mismatch`: Binary indicator of unusual location\n",
        "- `device_trust_score`: Device security score (0-100)\n",
        "- `velocity_last_24h`: Number of transactions in last 24 hours\n",
        "- `cardholder_age`: Age of cardholder\n",
        "\n",
        "**Target Variable:**\n",
        "- `is_fraud`: Binary classification (0=legitimate, 1=fraudulent)\n",
        "\n",
        "This is a publicly available, simulated dataset used strictly for academic purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9a490db",
      "metadata": {
        "id": "f9a490db"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Model building libraries\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
        ")\n",
        "\n",
        "# Set visualization style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "\n",
        "print(\"✓ All libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2353d83",
      "metadata": {
        "id": "c2353d83"
      },
      "source": [
        "## 2. Data Loading and Initial Inspection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f6fe0cc",
      "metadata": {
        "id": "8f6fe0cc"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('credit_card_fraud_10k.csv')\n",
        "\n",
        "print(f\"Dataset Shape: {data.shape}\")\n",
        "print(f\"\\nFirst 5 rows:\")\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "data_info",
      "metadata": {
        "id": "data_info"
      },
      "outputs": [],
      "source": [
        "# Dataset information\n",
        "print(\"Dataset Info:\")\n",
        "print(data.info())\n",
        "print(\"\\nMissing Values:\")\n",
        "print(data.isnull().sum())\n",
        "print(\"\\nBasic Statistics:\")\n",
        "data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22c5931d",
      "metadata": {
        "id": "22c5931d"
      },
      "outputs": [],
      "source": [
        "# Check class distribution\n",
        "fraud_counts = data['is_fraud'].value_counts()\n",
        "print(\"Class Distribution:\")\n",
        "print(fraud_counts)\n",
        "print(f\"\\nFraud Rate: {(fraud_counts[1] / len(data)) * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be300f98",
      "metadata": {
        "id": "be300f98"
      },
      "source": [
        "## 3. Data Visualization and Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34981d39",
      "metadata": {
        "id": "34981d39"
      },
      "outputs": [],
      "source": [
        "# Visualization 1: Class Distribution\n",
        "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Count plot\n",
        "sns.countplot(data=data, x='is_fraud', palette='Set2', ax=ax[0])\n",
        "ax[0].set_title('Transaction Class Distribution', fontsize=14, fontweight='bold')\n",
        "ax[0].set_xlabel('Transaction Type')\n",
        "ax[0].set_ylabel('Count')\n",
        "ax[0].set_xticklabels(['Legitimate', 'Fraudulent'])\n",
        "\n",
        "# Percentage pie chart\n",
        "fraud_counts.plot(kind='pie', autopct='%1.1f%%', labels=['Legitimate', 'Fraudulent'],\n",
        "                  colors=['#90EE90', '#FF6B6B'], ax=ax[1], startangle=90)\n",
        "ax[1].set_title('Fraud vs Legitimate Percentage', fontsize=14, fontweight='bold')\n",
        "ax[1].set_ylabel('')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1939054a",
      "metadata": {
        "id": "1939054a"
      },
      "outputs": [],
      "source": [
        "# Visualization 2: Transaction Amount Distribution\n",
        "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Histogram\n",
        "ax[0].hist(data[data['is_fraud'] == 0]['amount'], bins=50, alpha=0.7,\n",
        "           label='Legitimate', color='green', edgecolor='black')\n",
        "ax[0].hist(data[data['is_fraud'] == 1]['amount'], bins=50, alpha=0.7,\n",
        "           label='Fraudulent', color='red', edgecolor='black')\n",
        "ax[0].set_xlabel('Transaction Amount ($)')\n",
        "ax[0].set_ylabel('Frequency')\n",
        "ax[0].set_title('Transaction Amount Distribution', fontsize=14, fontweight='bold')\n",
        "ax[0].legend()\n",
        "\n",
        "# Box plot\n",
        "sns.boxplot(data=data, x='is_fraud', y='amount', palette='Set2', ax=ax[1])\n",
        "ax[1].set_xlabel('Transaction Type')\n",
        "ax[1].set_ylabel('Amount ($)')\n",
        "ax[1].set_title('Amount Distribution by Class', fontsize=14, fontweight='bold')\n",
        "ax[1].set_xticklabels(['Legitimate', 'Fraudulent'])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "viz_merchant",
      "metadata": {
        "id": "viz_merchant"
      },
      "outputs": [],
      "source": [
        "# Visualization 3: Fraud by Merchant Category\n",
        "fraud_by_merchant = data.groupby('merchant_category')['is_fraud'].agg(['sum', 'count'])\n",
        "fraud_by_merchant['fraud_rate'] = (fraud_by_merchant['sum'] / fraud_by_merchant['count']) * 100\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "fraud_by_merchant['fraud_rate'].plot(kind='bar', color='coral', edgecolor='black', ax=ax)\n",
        "ax.set_title('Fraud Rate by Merchant Category', fontsize=14, fontweight='bold')\n",
        "ax.set_xlabel('Merchant Category')\n",
        "ax.set_ylabel('Fraud Rate (%)')\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Fraud Rate by Merchant Category:\")\n",
        "print(fraud_by_merchant[['fraud_rate']].round(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "viz_time",
      "metadata": {
        "id": "viz_time"
      },
      "outputs": [],
      "source": [
        "# Visualization 4: Fraud by Hour of Day\n",
        "fraud_by_hour = data.groupby('transaction_hour')['is_fraud'].agg(['sum', 'count'])\n",
        "fraud_by_hour['fraud_rate'] = (fraud_by_hour['sum'] / fraud_by_hour['count']) * 100\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 6))\n",
        "ax.plot(fraud_by_hour.index, fraud_by_hour['fraud_rate'], marker='o',\n",
        "        linewidth=2, markersize=8, color='darkred')\n",
        "ax.fill_between(fraud_by_hour.index, fraud_by_hour['fraud_rate'], alpha=0.3, color='red')\n",
        "ax.set_title('Fraud Rate by Hour of Day', fontsize=14, fontweight='bold')\n",
        "ax.set_xlabel('Hour of Day')\n",
        "ax.set_ylabel('Fraud Rate (%)')\n",
        "ax.set_xticks(range(0, 24))\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "viz_correlation",
      "metadata": {
        "id": "viz_correlation"
      },
      "outputs": [],
      "source": [
        "# Visualization 5: Feature Correlation Heatmap\n",
        "# Encode categorical variables for correlation\n",
        "data_encoded = data.copy()\n",
        "le = LabelEncoder()\n",
        "data_encoded['merchant_category'] = le.fit_transform(data['merchant_category'])\n",
        "\n",
        "# Calculate correlation\n",
        "correlation_matrix = data_encoded.drop('transaction_id', axis=1).corr()\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm',\n",
        "            center=0, square=True, linewidths=1)\n",
        "plt.title('Feature Correlation Heatmap', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "data_engineering",
      "metadata": {
        "id": "data_engineering"
      },
      "source": [
        "## 4. Data Engineering and Preprocessing\n",
        "\n",
        "In this section, we prepare the data for machine learning by:\n",
        "1. Splitting features and target\n",
        "2. Creating train-test splits\n",
        "3. Building preprocessing pipelines\n",
        "4. Handling numerical and categorical features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09345b05",
      "metadata": {
        "id": "09345b05"
      },
      "outputs": [],
      "source": [
        "# Separate features and target\n",
        "X = data.drop(['is_fraud', 'transaction_id'], axis=1)\n",
        "y = data['is_fraud']\n",
        "\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")\n",
        "print(f\"\\nFeatures: {list(X.columns)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "preprocessing",
      "metadata": {
        "id": "preprocessing"
      },
      "outputs": [],
      "source": [
        "# Identify numerical and categorical features\n",
        "numerical_features = ['amount', 'transaction_hour', 'device_trust_score',\n",
        "                      'velocity_last_24h', 'cardholder_age']\n",
        "categorical_features = ['merchant_category']\n",
        "binary_features = ['foreign_transaction', 'location_mismatch']\n",
        "\n",
        "print(f\"Numerical features: {numerical_features}\")\n",
        "print(f\"Categorical features: {categorical_features}\")\n",
        "print(f\"Binary features: {binary_features}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "train_test",
      "metadata": {
        "id": "train_test"
      },
      "outputs": [],
      "source": [
        "# Create stratified train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]} ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"Test set size: {X_test.shape[0]} ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"\\nTraining set fraud rate: {y_train.mean()*100:.2f}%\")\n",
        "print(f\"Test set fraud rate: {y_test.mean()*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "preprocessing_pipeline",
      "metadata": {
        "id": "preprocessing_pipeline"
      },
      "outputs": [],
      "source": [
        "# Build preprocessing pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Define transformers\n",
        "numeric_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder(drop='first', sparse_output=False)\n",
        "\n",
        "# Create column transformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features),\n",
        "        ('bin', 'passthrough', binary_features)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "print(\"✓ Preprocessing pipeline created successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d298ec72",
      "metadata": {
        "id": "d298ec72"
      },
      "source": [
        "## 5. Model Architecture and Development\n",
        "\n",
        "We implement and compare three machine learning models:\n",
        "1. **Logistic Regression** - Baseline linear model\n",
        "2. **Random Forest** - Ensemble tree-based model\n",
        "3. **Gradient Boosting** - Advanced boosting model\n",
        "\n",
        "Each model is built using scikit-learn's Pipeline API for clean, reproducible code."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16bc4e64",
      "metadata": {
        "id": "16bc4e64"
      },
      "source": [
        "### 5.1 Logistic Regression (Baseline Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "570de33f",
      "metadata": {
        "id": "570de33f"
      },
      "outputs": [],
      "source": [
        "# Build Logistic Regression pipeline\n",
        "log_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced'))\n",
        "])\n",
        "\n",
        "print(\"Training Logistic Regression model...\")\n",
        "log_pipeline.fit(X_train, y_train)\n",
        "y_pred_log = log_pipeline.predict(X_test)\n",
        "y_pred_proba_log = log_pipeline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"✓ Logistic Regression model trained successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abc61e2f",
      "metadata": {
        "id": "abc61e2f"
      },
      "source": [
        "### 5.2 Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08ef2639",
      "metadata": {
        "id": "08ef2639"
      },
      "outputs": [],
      "source": [
        "# Build Random Forest pipeline\n",
        "rf_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=10,\n",
        "        min_samples_split=10,\n",
        "        min_samples_leaf=5,\n",
        "        random_state=42,\n",
        "        class_weight='balanced',\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "print(\"Training Random Forest model...\")\n",
        "rf_pipeline.fit(X_train, y_train)\n",
        "y_pred_rf = rf_pipeline.predict(X_test)\n",
        "y_pred_proba_rf = rf_pipeline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"✓ Random Forest model trained successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gb_model",
      "metadata": {
        "id": "gb_model"
      },
      "source": [
        "### 5.3 Gradient Boosting Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gb_train",
      "metadata": {
        "id": "gb_train"
      },
      "outputs": [],
      "source": [
        "# Build Gradient Boosting pipeline\n",
        "gb_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', GradientBoostingClassifier(\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=5,\n",
        "        min_samples_split=10,\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "print(\"Training Gradient Boosting model...\")\n",
        "gb_pipeline.fit(X_train, y_train)\n",
        "y_pred_gb = gb_pipeline.predict(X_test)\n",
        "y_pred_proba_gb = gb_pipeline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"✓ Gradient Boosting model trained successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d2b9ba9",
      "metadata": {
        "id": "9d2b9ba9"
      },
      "source": [
        "## 6. Model Evaluation and Performance Metrics\n",
        "\n",
        "We evaluate all models using comprehensive metrics:\n",
        "- **Accuracy**: Overall correctness\n",
        "- **Precision**: Fraud prediction accuracy\n",
        "- **Recall**: Fraud detection rate\n",
        "- **F1-Score**: Harmonic mean of precision and recall\n",
        "- **ROC-AUC**: Area under the ROC curve\n",
        "- **Confusion Matrix**: Detailed error analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1cc621c",
      "metadata": {
        "id": "b1cc621c"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(y_true, y_pred, y_pred_proba, model_name):\n",
        "    \"\"\"Comprehensive model evaluation function\"\"\"\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Performance Metrics for {model_name}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
        "\n",
        "    print(f\"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "    print(f\"Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
        "    print(f\"Recall:    {recall:.4f} ({recall*100:.2f}%)\")\n",
        "    print(f\"F1-Score:  {f1:.4f}\")\n",
        "    print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "    print(\"\\nDetailed Breakdown:\")\n",
        "    print(f\"True Negatives:  {cm[0, 0]:>5} (Correctly identified legitimate)\")\n",
        "    print(f\"False Positives: {cm[0, 1]:>5} (Legitimate flagged as fraud)\")\n",
        "    print(f\"False Negatives: {cm[1, 0]:>5} (Fraud missed)\")\n",
        "    print(f\"True Positives:  {cm[1, 1]:>5} (Correctly detected fraud)\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=['Legitimate', 'Fraudulent']))\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'roc_auc': roc_auc,\n",
        "        'confusion_matrix': cm\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cf7e855",
      "metadata": {
        "id": "0cf7e855"
      },
      "outputs": [],
      "source": [
        "# Evaluate Logistic Regression\n",
        "metrics_log = evaluate_model(y_test, y_pred_log, y_pred_proba_log, \"Logistic Regression\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d0d5f00",
      "metadata": {
        "id": "8d0d5f00"
      },
      "outputs": [],
      "source": [
        "# Evaluate Random Forest\n",
        "metrics_rf = evaluate_model(y_test, y_pred_rf, y_pred_proba_rf, \"Random Forest Classifier\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eval_gb",
      "metadata": {
        "id": "eval_gb"
      },
      "outputs": [],
      "source": [
        "# Evaluate Gradient Boosting\n",
        "metrics_gb = evaluate_model(y_test, y_pred_gb, y_pred_proba_gb, \"Gradient Boosting Classifier\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "model_comparison",
      "metadata": {
        "id": "model_comparison"
      },
      "source": [
        "## 7. Model Comparison and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "compare_models",
      "metadata": {
        "id": "compare_models"
      },
      "outputs": [],
      "source": [
        "# Create comparison dataframe\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': ['Logistic Regression', 'Random Forest', 'Gradient Boosting'],\n",
        "    'Accuracy': [metrics_log['accuracy'], metrics_rf['accuracy'], metrics_gb['accuracy']],\n",
        "    'Precision': [metrics_log['precision'], metrics_rf['precision'], metrics_gb['precision']],\n",
        "    'Recall': [metrics_log['recall'], metrics_rf['recall'], metrics_gb['recall']],\n",
        "    'F1-Score': [metrics_log['f1_score'], metrics_rf['f1_score'], metrics_gb['f1_score']],\n",
        "    'ROC-AUC': [metrics_log['roc_auc'], metrics_rf['roc_auc'], metrics_gb['roc_auc']]\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL COMPARISON SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_df.to_string(index=False))\n",
        "print(\"=\"*80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "viz_comparison",
      "metadata": {
        "id": "viz_comparison"
      },
      "outputs": [],
      "source": [
        "# Visualize model comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Metrics comparison bar plot\n",
        "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "x = np.arange(len(comparison_df))\n",
        "width = 0.2\n",
        "\n",
        "for idx, metric in enumerate(metrics_to_plot):\n",
        "    row = idx // 2\n",
        "    col = idx % 2\n",
        "\n",
        "    axes[row, col].bar(x, comparison_df[metric], width=0.6,\n",
        "                       color=['#3498db', '#2ecc71', '#e74c3c'])\n",
        "    axes[row, col].set_ylabel(metric, fontsize=12)\n",
        "    axes[row, col].set_title(f'{metric} Comparison', fontsize=14, fontweight='bold')\n",
        "    axes[row, col].set_xticks(x)\n",
        "    axes[row, col].set_xticklabels(comparison_df['Model'], rotation=15, ha='right')\n",
        "    axes[row, col].set_ylim([0, 1.1])\n",
        "    axes[row, col].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for i, v in enumerate(comparison_df[metric]):\n",
        "        axes[row, col].text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "viz_roc",
      "metadata": {
        "id": "viz_roc"
      },
      "outputs": [],
      "source": [
        "# ROC Curves for all models\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Logistic Regression ROC\n",
        "fpr_log, tpr_log, _ = roc_curve(y_test, y_pred_proba_log)\n",
        "plt.plot(fpr_log, tpr_log, label=f'Logistic Regression (AUC = {metrics_log[\"roc_auc\"]:.3f})',\n",
        "         linewidth=2, color='#3498db')\n",
        "\n",
        "# Random Forest ROC\n",
        "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_proba_rf)\n",
        "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {metrics_rf[\"roc_auc\"]:.3f})',\n",
        "         linewidth=2, color='#2ecc71')\n",
        "\n",
        "# Gradient Boosting ROC\n",
        "fpr_gb, tpr_gb, _ = roc_curve(y_test, y_pred_proba_gb)\n",
        "plt.plot(fpr_gb, tpr_gb, label=f'Gradient Boosting (AUC = {metrics_gb[\"roc_auc\"]:.3f})',\n",
        "         linewidth=2, color='#e74c3c')\n",
        "\n",
        "# Diagonal line (random classifier)\n",
        "plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')\n",
        "\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('ROC Curves - Model Comparison', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right', fontsize=11)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "viz_confusion",
      "metadata": {
        "id": "viz_confusion"
      },
      "outputs": [],
      "source": [
        "# Confusion matrices visualization\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "cms = [metrics_log['confusion_matrix'],\n",
        "       metrics_rf['confusion_matrix'],\n",
        "       metrics_gb['confusion_matrix']]\n",
        "titles = ['Logistic Regression', 'Random Forest', 'Gradient Boosting']\n",
        "\n",
        "for idx, (cm, title) in enumerate(zip(cms, titles)):\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "                xticklabels=['Legitimate', 'Fraudulent'],\n",
        "                yticklabels=['Legitimate', 'Fraudulent'],\n",
        "                ax=axes[idx])\n",
        "    axes[idx].set_title(f'{title}\\nConfusion Matrix', fontsize=12, fontweight='bold')\n",
        "    axes[idx].set_ylabel('Actual', fontsize=11)\n",
        "    axes[idx].set_xlabel('Predicted', fontsize=11)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feature_importance",
      "metadata": {
        "id": "feature_importance"
      },
      "source": [
        "## 8. Feature Importance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feat_importance",
      "metadata": {
        "id": "feat_importance"
      },
      "outputs": [],
      "source": [
        "# Extract feature importance from Random Forest (best performing model)\n",
        "feature_names = (numerical_features +\n",
        "                 list(rf_pipeline.named_steps['preprocessor']\n",
        "                      .named_transformers_['cat']\n",
        "                      .get_feature_names_out(categorical_features)) +\n",
        "                 binary_features)\n",
        "\n",
        "importances = rf_pipeline.named_steps['classifier'].feature_importances_\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': importances\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(feature_importance_df['feature'], feature_importance_df['importance'],\n",
        "         color='steelblue', edgecolor='black')\n",
        "plt.xlabel('Importance', fontsize=12)\n",
        "plt.ylabel('Feature', fontsize=12)\n",
        "plt.title('Feature Importance (Random Forest)', fontsize=14, fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTop 10 Most Important Features:\")\n",
        "print(feature_importance_df.head(10).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "model_save",
      "metadata": {
        "id": "model_save"
      },
      "source": [
        "## 9. Model Persistence for Deployment\n",
        "\n",
        "We save the best-performing model for deployment in the web application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "save_model",
      "metadata": {
        "id": "save_model"
      },
      "outputs": [],
      "source": [
        "# Determine best model based on F1-score (balanced metric for imbalanced data)\n",
        "best_model_name = comparison_df.loc[comparison_df['F1-Score'].idxmax(), 'Model']\n",
        "\n",
        "if best_model_name == 'Logistic Regression':\n",
        "    best_model = log_pipeline\n",
        "elif best_model_name == 'Random Forest':\n",
        "    best_model = rf_pipeline\n",
        "else:\n",
        "    best_model = gb_pipeline\n",
        "\n",
        "# Save the best model\n",
        "model_filename = 'fraud_detection_model.pkl'\n",
        "joblib.dump(best_model, model_filename)\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"BEST MODEL: {best_model_name}\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"✓ Model saved as: {model_filename}\")\n",
        "print(f\"✓ File size: {os.path.getsize(model_filename) / 1024:.2f} KB\")\n",
        "print(f\"✓ Ready for deployment!\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "# Save feature names for deployment\n",
        "feature_info = {\n",
        "    'numerical_features': numerical_features,\n",
        "    'categorical_features': categorical_features,\n",
        "    'binary_features': binary_features,\n",
        "    'all_features': list(X.columns)\n",
        "}\n",
        "joblib.dump(feature_info, 'feature_info.pkl')\n",
        "print(\"✓ Feature information saved for deployment\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52ecd4d5",
      "metadata": {
        "id": "52ecd4d5"
      },
      "source": [
        "## 10. Conclusion and Next Steps\n",
        "\n",
        "### Summary of Results\n",
        "\n",
        "This notebook demonstrated a complete machine learning pipeline for fraud detection:\n",
        "\n",
        "1. **Data Exploration**: Analyzed 10,000 transactions with comprehensive visualizations\n",
        "2. **Feature Engineering**: Created preprocessing pipelines for numerical and categorical features\n",
        "3. **Model Development**: Implemented and compared three ML algorithms\n",
        "4. **Performance Evaluation**: Achieved strong results across all metrics\n",
        "5. **Model Deployment**: Saved best-performing model for production use\n",
        "\n",
        "### Key Findings\n",
        "\n",
        "- All models demonstrated strong fraud detection capabilities\n",
        "- The best model achieved excellent performance on imbalanced data\n",
        "- Feature importance analysis revealed key fraud indicators\n",
        "- Model is ready for deployment via web interface\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Web Application Deployment**: Create Flask API for real-time predictions\n",
        "2. **User Interface**: Build interactive dashboard for fraud monitoring\n",
        "3. **Model Monitoring**: Implement performance tracking in production\n",
        "4. **Continuous Improvement**: Collect feedback and retrain models\n",
        "\n",
        "---\n",
        "\n",
        "**Project Repository**: [GitHub Link]  \n",
        "**Demo Video**: [Video Link]  \n",
        "**Author**: Armand Kayiranga  \n",
        "**Date**: February 2026"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}